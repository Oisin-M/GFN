{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "pname = 'stokes'\n",
    "name_mesh = 'reference_mesh'\n",
    "name_matrix = 'matrix'\n",
    "n_params = 7\n",
    "\n",
    "train_fidelities = ['_large']\n",
    "# train_fidelities = ['_large', '_medium', '_small', '_tiny']\n",
    "\n",
    "test_fidelities = ['_large', '_medium', '_small', '_tiny']\n",
    "\n",
    "# NB: order training meshes by largest first\n",
    "train_mesh_names = [name_mesh+''.join(f)+'.csv' for f in train_fidelities]\n",
    "train_solution_names = [name_matrix+''.join(f)+'.mat' for f in train_fidelities]\n",
    "\n",
    "test_mesh_names = [name_mesh+''.join(f)+'.csv' for f in test_fidelities]\n",
    "test_solution_names = [name_matrix+''.join(f)+'.mat' for f in test_fidelities]\n",
    "\n",
    "joined = ''.join(train_fidelities)\n",
    "\n",
    "save_name = 'best_model'+joined+'.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc115edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykdtree.kdtree import KDTree\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import colormaps\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "plt_params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "precision = torch.float64\n",
    "torch.set_default_dtype(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9f867",
   "metadata": {},
   "source": [
    "## THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60269142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFN_AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing the GFN method for the encoder and decoder architectures.\n",
    "    Methods:\n",
    "    __init__: initialises the master mesh, and master weight & biases for the 1st and last layer of the encoder and decoder, respectively\n",
    "    expand: add all new expansive nodes in the new mesh to the weight matrices\n",
    "    agglomerate: agglomerate nodes to fit the new mesh\n",
    "    encoder: execute the encoder part\n",
    "    decoder: execute the decoder part\n",
    "    \"\"\"\n",
    "    def __init__(self, mesh_m, latent_size=20):\n",
    "        super().__init__()\n",
    "        size = mesh_m.shape[0]\n",
    "        self.latent_size = latent_size\n",
    "        self.We_m = nn.Parameter(torch.empty(size, self.latent_size))\n",
    "        self.be_m = nn.Parameter(torch.empty(self.latent_size))\n",
    "        self.Wd_m = nn.Parameter(torch.empty(self.latent_size, size))\n",
    "        self.bd_m = nn.Parameter(torch.empty(size))\n",
    "        self.mesh_m = mesh_m\n",
    "        \n",
    "        self.initialise(self.We_m, self.be_m)\n",
    "        self.initialise(self.Wd_m, self.bd_m)\n",
    "        \n",
    "        # Note: no self.be_n since we never need to reshape the encoder biases i.e. be_n == be_m in all cases\n",
    "        self.We_n = self.We_m.clone()\n",
    "        self.Wd_n = self.Wd_m.clone()\n",
    "        self.bd_n = self.bd_m.clone()\n",
    "        self.mesh_n = self.mesh_m\n",
    "        \n",
    "    def initialise(self, weight, bias):\n",
    "        stdv = 1. / math.sqrt(weight.size(1))\n",
    "        weight.data.uniform_(-stdv, stdv)\n",
    "        bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def expand(self, mesh_n, kd_tree_m=None, kd_tree_n=None, nn_m=None, nn_n=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Expand the new weights and biases with the new expansive nodes. If during training, update the master weights, biases & mesh.\n",
    "        If during inference, then just apply the new ones.\n",
    "        \"\"\"\n",
    "        \n",
    "        # During evaluation, we will have fixed mesh_m so we don't want to recompute every time => option to pass in\n",
    "        if nn_n is not None:\n",
    "            pass\n",
    "        elif kd_tree_n is not None:\n",
    "            nn_n = kd_tree_m.query(mesh_n, k=1)[1]\n",
    "        else:\n",
    "            kd_tree_m = KDTree(self.mesh_m)\n",
    "            nn_n = kd_tree_m.query(mesh_n, k=1)[1]\n",
    "        # During training, we will have fixed fidelities that we know and don't need to always recompute\n",
    "        if nn_m is not None:\n",
    "            pass\n",
    "        elif kd_tree_n is not None:\n",
    "            nn_m = kd_tree_n.query(self.mesh_m, k=1)[1]\n",
    "        else:\n",
    "            kd_tree_n = KDTree(mesh_n)\n",
    "            nn_m = kd_tree_n.query(self.mesh_m, k=1)[1]\n",
    "\n",
    "        if not self.training:\n",
    "            # ! testing mode !\n",
    "            \n",
    "            count_m = np.zeros(self.mesh_m.shape[0]) # track how many neighbours\n",
    "            nodes_added = 0 # how much did we increase our master mesh\n",
    "\n",
    "            for pt_n, pt_m in enumerate(nn_n):\n",
    "                if nn_m[pt_m]!=pt_n: # if not bidirectional link <->\n",
    "                    nodes_added += 1\n",
    "                    self.mesh_n = np.vstack([self.mesh_n, mesh_n[pt_n]])\n",
    "                    count_m[pt_m]+=1\n",
    "\n",
    "                    # Divide encoder weights by number of expansions\n",
    "                    self.We_n[pt_m]*=count_m[pt_m]/(count_m[pt_m]+1)\n",
    "                    # Store the index of the weight we want\n",
    "                    # so we can update at the end without storing\n",
    "                    # all the nodes to update explictly\n",
    "                    new_row = torch.zeros(1, self.We_n.shape[1])\n",
    "                    new_row[0][0] = pt_m\n",
    "                    self.We_n = torch.cat((self.We_n, new_row), dim=0)\n",
    "\n",
    "                    # Duplicate weights for decoder\n",
    "                    self.Wd_n = torch.cat((self.Wd_n, self.Wd_n[:, pt_m:pt_m+1]), dim=1)\n",
    "                    self.bd_n = torch.cat([self.bd_n, self.bd_n[pt_m:pt_m+1]])\n",
    "\n",
    "            # Loop over the nodes we need to update using the index we stored in the first element\n",
    "            for i in range(self.mesh_m.shape[0], self.mesh_m.shape[0]+nodes_added):\n",
    "                index = int(self.We_n[i,0])\n",
    "                self.We_n[i] = self.We_n[index]\n",
    "        else:\n",
    "            # ! training mode !\n",
    "            \n",
    "            # Expansion step is essentially creating new weights from scratch\n",
    "            # => ignore gradients and therefore allow for slicing on leaf tensor as required\n",
    "            # (Unless we do something smarter with the gradient tree to track what we're doing...)\n",
    "            with torch.no_grad():\n",
    "                count_m = np.zeros(self.mesh_m.shape[0]) # track how many neighbours\n",
    "                nodes_added = 0 # how much did we increase our master mesh\n",
    "                size=self.mesh_m.shape[0]\n",
    "\n",
    "                for pt_n, pt_m in enumerate(nn_n):\n",
    "                    if nn_m[pt_m]!=pt_n: # if not bidirectional link <->\n",
    "                        nodes_added += 1\n",
    "                        self.mesh_m=np.vstack([self.mesh_m, mesh_n[pt_n]])\n",
    "                        count_m[pt_m]+=1\n",
    "                        \n",
    "                        # Divide encoder weights by number of expansions\n",
    "                        self.We_m[pt_m]*=count_m[pt_m]/(count_m[pt_m]+1)\n",
    "                        # Store the index of the weight we want\n",
    "                        # so we can update at the end without storing\n",
    "                        # all the nodes to update explictly\n",
    "                        new_row = torch.zeros(1, self.We_m.shape[1])\n",
    "                        new_row[0][0] = pt_m\n",
    "                        self.We_m = nn.Parameter(torch.cat((self.We_m, new_row), dim=0))\n",
    "                        \n",
    "                        # Duplicate weights for decoder\n",
    "                        self.Wd_m = nn.Parameter(torch.cat((self.Wd_m, self.Wd_m[:, pt_m:pt_m+1]), dim=1))\n",
    "                        self.bd_m = nn.Parameter(torch.cat([self.bd_m, self.bd_m[pt_m:pt_m+1]]))\n",
    "                \n",
    "                # Loop over the nodes we need to update using the index we stored in the first element\n",
    "                for i in range(size, size+nodes_added):\n",
    "                    index = int(self.We_m[i,0])\n",
    "                    self.We_m[i] = self.We_m[index]\n",
    "            \n",
    "            # now we need to track gradients from our new master weights\n",
    "            self.We_n = self.We_m.clone()\n",
    "            self.Wd_n = self.Wd_m.clone()\n",
    "            self.bd_n = self.bd_m.clone()\n",
    "        return nodes_added\n",
    "        \n",
    "    def agglomerate(self, mesh_n, kd_tree_n=None, nn_m=None, **kwargs):\n",
    "        \n",
    "        # known during training\n",
    "        if nn_m is not None:\n",
    "            pass\n",
    "        # already found in expansion or already known during training\n",
    "        elif kd_tree_n is not None:\n",
    "            nn_m = kd_tree_n.query(self.mesh_n, k=1)[1]\n",
    "        else:\n",
    "            kd_tree_n = KDTree(mesh_n)\n",
    "            nn_m = kd_tree_n.query(self.mesh_n, k=1)[1]\n",
    "\n",
    "        # FIND NEW WEIGHTS (AGGLOMERATIVE ONLY NOW)\n",
    "        We_n = torch.zeros((mesh_n.shape[0], self.We_n.shape[1]), device=self.We_n.device)\n",
    "        Wd_n = torch.zeros((self.We_n.shape[1], mesh_n.shape[0]), device=self.We_n.device)\n",
    "        bd_n = torch.zeros(mesh_n.shape[0], device=self.We_n.device)\n",
    "\n",
    "        count_n = np.zeros(mesh_n.shape[0])\n",
    "\n",
    "        for m, pt_m in enumerate(self.mesh_n): # agglomerate\n",
    "            nn_n = nn_m[m]\n",
    "            count_n[nn_n]+=1\n",
    "            We_n[nn_n] += self.We_n[m]\n",
    "            Wd_n[:, nn_n] = ( (count_n[nn_n]-1)*Wd_n[:, nn_n] + self.Wd_n[:, m] )/count_n[nn_n]\n",
    "            bd_n[nn_n] = ( (count_n[nn_n]-1)*bd_n[nn_n] + self.bd_n[m] )/count_n[nn_n]\n",
    "          \n",
    "        self.We_n = We_n\n",
    "        self.Wd_n = Wd_n\n",
    "        self.bd_n = bd_n\n",
    "        \n",
    "    def reset(self):\n",
    "        self.We_n = self.We_m.clone()\n",
    "        self.Wd_n = self.Wd_m.clone()\n",
    "        self.bd_n = self.bd_m.clone()\n",
    "        if not self.training:\n",
    "            self.mesh_n = self.mesh_m\n",
    "        \n",
    "    def encoder(self, x, mesh_n, exp_enc=True, agg_enc=True, reset_enc=False, **kwargs):\n",
    "        if reset_enc:\n",
    "            self.reset()\n",
    "        if exp_enc:\n",
    "            self.expand(mesh_n, **kwargs)\n",
    "        if agg_enc:\n",
    "            self.agglomerate(mesh_n, **kwargs)\n",
    "        return x@self.We_n+self.be_m\n",
    "        \n",
    "    def decoder(self, x, mesh_n, exp_dec=True, agg_dec=True, reset_dec=False, **kwargs):\n",
    "        if reset_dec:\n",
    "            self.reset()\n",
    "        if exp_dec:\n",
    "            self.expand(mesh_n, **kwargs)\n",
    "        if agg_dec:\n",
    "            self.agglomerate(mesh_n, **kwargs)\n",
    "        return x@self.Wd_n+self.bd_n\n",
    "    \n",
    "class GCA(nn.Module):\n",
    "    \n",
    "    def __init__(self, mesh_m, gfn_latent_size=20, latent_size=20, n_params=7, act=nn.Tanh, ae_sizes=[], map_sizes=[50]*4):\n",
    "        super().__init__()\n",
    "        self.GFN = GFN_AE(mesh_m, gfn_latent_size)\n",
    "        \n",
    "        self.act = act()\n",
    "        \n",
    "        module_list_enc = []\n",
    "        module_list_dec = []\n",
    "\n",
    "        for i in range(len(ae_sizes)):\n",
    "            module_list_dec.append(self.act)\n",
    "            if i==0:\n",
    "                module_list_enc.append(nn.Linear(gfn_latent_size, ae_sizes[i]))\n",
    "                module_list_dec.append(nn.Linear(ae_sizes[i], gfn_latent_size))\n",
    "            else:\n",
    "                module_list_enc.append(nn.Linear(ae_sizes[i-1], ae_sizes[i]))\n",
    "                module_list_dec.append(nn.Linear(ae_sizes[i], ae_sizes[i-1]))\n",
    "            module_list_enc.append(self.act)\n",
    "        if len(ae_sizes)!=0:\n",
    "            module_list_dec.append(self.act)\n",
    "            module_list_enc.append(nn.Linear(ae_sizes[-1], latent_size))\n",
    "            module_list_dec.append(nn.Linear(latent_size, ae_sizes[-1]))\n",
    "            module_list_enc.append(self.act)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*module_list_enc)\n",
    "        self.decoder = nn.Sequential(*module_list_dec[::-1])\n",
    "        \n",
    "        module_list_map = []\n",
    "        \n",
    "        for i in range(len(map_sizes)):\n",
    "            if i==0:\n",
    "                module_list_map.append(nn.Linear(n_params, map_sizes[i]))\n",
    "            else:\n",
    "                module_list_map.append(nn.Linear(map_sizes[i-1], map_sizes[i]))\n",
    "            module_list_map.append(act())\n",
    "        if len(map_sizes)!=0:\n",
    "            module_list_map.append(nn.Linear(map_sizes[-1], latent_size))\n",
    "            \n",
    "        self.mapper = nn.Sequential(*module_list_map)\n",
    "        \n",
    "    def forward(self, x, mesh_n, params, **kwargs):\n",
    "        x_enc = self.act(self.GFN.encoder(x, mesh_n, **kwargs))\n",
    "        x_enc = self.encoder(x_enc)\n",
    "        \n",
    "        x_map = self.mapper(params)\n",
    "        \n",
    "        x_recon = self.decoder(x_enc)\n",
    "        x_recon = self.GFN.decoder(x_enc, mesh_n, **kwargs)\n",
    "        \n",
    "        return x_recon, x_enc, x_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b67f68",
   "metadata": {},
   "source": [
    "## SET UP TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826652a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(pname='stokes'):\n",
    "    if pname=='stokes':\n",
    "        mu_range = [(0.5,1.5), (0.5,1.5), (0.5,1.5), (0.5,1.5), (0.5,1.5), (-np.pi/6,np.pi/6), (-10,10)]\n",
    "        mus = []\n",
    "        n_pts = [2]*(len(mu_range)-1)+[11]\n",
    "        for i in range(len(mu_range)):\n",
    "            mus.append(np.linspace(mu_range[i][0], mu_range[i][1], n_pts[i]))\n",
    "        return torch.tensor(np.array(list(product(*mus))), dtype=precision)\n",
    "    elif pname=='graetz':\n",
    "        mus = [np.linspace(1., 3., 10), np.linspace(0.01, 0.1, 20)]\n",
    "        mu1, mu2 = np.meshgrid(mus[0], mus[1])\n",
    "        return torch.tensor(np.vstack((mu1.T, mu2.T)).reshape(2, -1).T, dtype=precision)\n",
    "    elif pname=='advection':\n",
    "        mus = [np.linspace(0., 6., 10), np.linspace(-1.0, 1.0, 10)]\n",
    "        mu1, mu2 = np.meshgrid(mus[0], mus[1])\n",
    "        return torch.tensor(np.vstack((mu1.T, mu2.T)).reshape(2, -1).T, dtype=precision)\n",
    "\n",
    "def scaler_func():\n",
    "    return preprocessing.StandardScaler()\n",
    "\n",
    "def scaling(U):\n",
    "    scaling_fun_1 = scaler_func()\n",
    "    scaling_fun_2 = scaler_func()\n",
    "    scaler_s = scaling_fun_1.fit(U)\n",
    "    temp = torch.t(torch.tensor(scaler_s.transform(U)))\n",
    "    scaler_f = scaling_fun_2.fit(temp)\n",
    "    scaled_data = torch.unsqueeze(torch.t(torch.tensor(scaler_f.transform(temp), dtype=precision)),0).permute(2, 1, 0)\n",
    "    scale = [scaler_s, scaler_f]\n",
    "    return scale, scaled_data[:,:,0]\n",
    "\n",
    "def undo_scaling(U, scale):\n",
    "    scaler_s = scale[0]\n",
    "    scaler_f = scale[1]\n",
    "    rescaled_data = torch.tensor(scaler_s.inverse_transform(torch.t(torch.tensor(scaler_f.inverse_transform(U.detach().numpy().squeeze())))), dtype=precision)\n",
    "    return rescaled_data\n",
    "\n",
    "def get_scaled_data(fname):\n",
    "    U_orig = torch.tensor(scipy.io.loadmat(\"dataset/\"+fname)['U'])\n",
    "    scale, U_sc = scaling(U_orig)\n",
    "    return scale, U_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params(pname).to(dev)\n",
    "\n",
    "dfs_train = [pd.read_csv(\"dataset/\"+i, header=None).values for i in train_mesh_names]\n",
    "\n",
    "sols_train = [get_scaled_data(i)[1].to(dev) for i in train_solution_names]\n",
    "\n",
    "trajs = list(range(sols_train[0].shape[0]))\n",
    "random.shuffle(trajs)\n",
    "train_trajs, test_trajs = np.array_split(trajs, 2)\n",
    "train_trajs = np.array_split(train_trajs, len(dfs_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6adfe",
   "metadata": {},
   "source": [
    "## TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation\n",
    "# NB - important to start with largest mesh since this is better for initialisation purposes\n",
    "# and potential savings in no expansions are done\n",
    "model = GCA(dfs_train[0], n_params=n_params).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091eeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(\"models/best_model\"+joined+\".pt\"))\n",
    "    print(\"Loading saved network\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Training network\")\n",
    "    # conduct the expansion step\n",
    "    added_nodes = 0\n",
    "    for df in dfs_train[1:]:\n",
    "        added_nodes += model.GFN.expand(df)\n",
    "\n",
    "    print(\"Number of added nodes: \", added_nodes)\n",
    "    model.train()\n",
    "    # now initialise our optimiser after we have conducted ALL expansions\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "    best_loss = np.inf\n",
    "    EPOCHS = 5000\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "\n",
    "    def criterion(x, x_recon, x_enc, x_map):\n",
    "        return nn.functional.mse_loss(x, x_recon)+1e1*nn.functional.mse_loss(x_enc, x_map)\n",
    "\n",
    "    kd_tree_m = KDTree(model.GFN.mesh_m)\n",
    "    # we know ahead of time the nn_n and nn_ms since we know that the master mesh is never changing and our new meshes are fixed\n",
    "    nn_ns = [kd_tree_m.query(df, k=1)[1] for df in dfs_train]\n",
    "    nn_ms = [KDTree(df).query(model.GFN.mesh_m, k=1)[1] for df in dfs_train]\n",
    "\n",
    "    loop = tqdm(range(EPOCHS))\n",
    "    for i in loop:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        U_train = sols_train[0][train_trajs[0]]\n",
    "        params_train = params[train_trajs[0]]\n",
    "        df_train = dfs_train[0]\n",
    "        if added_nodes==0:\n",
    "            # because added_nodes==0, we know we don't need to expand or agglom for the medium mesh\n",
    "            x_recon, x_enc, x_map = model(U_train, df_train, params_train, exp_enc=False, exp_dec=False, agg_enc=False, agg_dec=False, reset_enc=True)\n",
    "        else:\n",
    "            # NEED TO TEST\n",
    "            # we need to agglomerate because added_nodes>0. However, we do this all in the encoder first so we can avoid in the decoder\n",
    "            x_recon, x_enc, x_map = model(U_train, df_train, params_train, exp_enc=False, exp_dec=False, agg_dec=False, reset_enc=True, nn_n=nn_ns[0], nn_m=nn_ms[0])\n",
    "\n",
    "        loss = criterion(U_train, x_recon, x_enc, x_map)*df_train.shape[0]\n",
    "\n",
    "        # other fidelities\n",
    "        for j in range(1, len(dfs_train)):\n",
    "            U_train = sols_train[j][train_trajs[j]]\n",
    "            params_train = params[train_trajs[j]]\n",
    "            df_train = dfs_train[j]\n",
    "            \n",
    "            # only need to agglom in training since we expanded as a preprocessing step\n",
    "            x_recon, x_enc, x_map = model(U_train, df_train, params_train, exp_enc=False, exp_dec=False, agg_dec=False, reset_enc=True, nn_m=nn_ms[j], nn_n=nn_ns[j])\n",
    "            loss += criterion(U_train, x_recon, x_enc, x_map)*df_train.shape[0]\n",
    "        \n",
    "        loss /= np.sum([k.shape[0] for k in dfs_train])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params_test = params[test_trajs]\n",
    "            \n",
    "            U_test = sols_train[0][test_trajs]\n",
    "            df_test = dfs_train[0]\n",
    "\n",
    "            if added_nodes==0:\n",
    "                x_recon, x_enc, x_map = model(U_test, df_test, params_test, exp_enc=False, exp_dec=False, agg_enc=False, agg_dec=False, reset_enc=True)\n",
    "            else:\n",
    "                x_recon, x_enc, x_map = model(U_test, df_test, params_test, exp_enc=False, exp_dec=False, agg_dec=False, reset_enc=True, nn_n=nn_ns[0], nn_m=nn_ms[0])\n",
    "            test_loss = criterion(U_test, x_recon, x_enc, x_map).item()*df_test.shape[0]\n",
    "\n",
    "            for j in range(1, len(dfs_train)):\n",
    "                U_test = sols_train[j][test_trajs]\n",
    "                df_test = dfs_train[j]\n",
    "                \n",
    "                # only need to agglom in training since we expanded as a preprocessing step\n",
    "                x_recon, x_enc, x_map = model(U_test, df_test, params_test, exp_enc=False, exp_dec=False, agg_dec=False, reset_enc=True, nn_m=nn_ms[j], nn_n=nn_ns[j])\n",
    "                test_loss += criterion(U_test, x_recon, x_enc, x_map).item()*df_train.shape[0]\n",
    "            \n",
    "            test_loss /= np.sum([k.shape[0] for k in dfs_train])\n",
    "        \n",
    "        loop.set_postfix({\"Loss(training)\": train_loss, \"Loss(testing)\": test_loss})\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        if test_loss<best_loss:\n",
    "            best_loss=test_loss\n",
    "            best_epoch = i\n",
    "            torch.save(model.state_dict(), \"models/\"+save_name)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"models/best_model\"+joined+\".pt\"))\n",
    "    \n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(test_losses, label='test')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig('plots/loss_plot'+joined+'.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984248e4",
   "metadata": {},
   "source": [
    "## EVALUATE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721713de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(model, df_large, U_large, scale, params):\n",
    "    with torch.no_grad():\n",
    "        x_recon, x_enc, x_map = model(U_large, df_large, params, reset_enc=True, reset_dec=True)\n",
    "        x_rom = model.decoder(x_map)\n",
    "        x_rom = model.GFN.decoder(x_rom, df_large, exp_enc=False, exp_dec=False, agg_enc=False, agg_dec=False)\n",
    "        Z = undo_scaling(U_large, scale)\n",
    "        Z_net = undo_scaling(x_rom, scale)\n",
    "    return Z, Z_net, x_enc, x_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(Z, Z_net, x_enc, x_map):\n",
    "    error_abs_list = list()\n",
    "    norm_z_list = list()\n",
    "    latents_error = list()\n",
    "\n",
    "    for snap in range(Z.shape[1]):\n",
    "        error_abs = np.linalg.norm(abs(Z[:, snap] - Z_net[:, snap]))\n",
    "        norm_z = np.linalg.norm(Z[:, snap], 2)\n",
    "        error_abs_list.append(error_abs)\n",
    "        norm_z_list.append(norm_z)\n",
    "        lat_err = np.linalg.norm(x_enc[snap] - x_map[snap])/np.linalg.norm(x_enc[snap])\n",
    "        latents_error.append(lat_err)\n",
    "\n",
    "    latents_error = np.array(latents_error)\n",
    "    print(\"\\nMaximum relative error for latent  = \", max(latents_error))\n",
    "    print(\"Mean relative error for latent = \", sum(latents_error)/len(latents_error))\n",
    "    print(\"Minimum relative error for latent = \", min(latents_error))\n",
    "\n",
    "    error = np.array(error_abs_list)\n",
    "    norm = np.array(norm_z_list)\n",
    "    rel_error = error/norm\n",
    "    print(\"\\nMaximum absolute error for field \"+\" = \", max(error))\n",
    "    print(\"Mean absolute error for field \"+\" = \", sum(error)/len(error))\n",
    "    print(\"Minimum absolute error for field \"+\" = \", min(error))\n",
    "    print(\"\\nMaximum relative error for field \"+\" = \", max(rel_error))\n",
    "    print(\"Mean relative error for field \"+\" = \", sum(rel_error)/len(rel_error))\n",
    "    print(\"Minimum relative error for field \"+\" = \", min(rel_error))\n",
    "    return rel_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fields(SNAP, Z, xx, yy, triang, params, fname):\n",
    "    fig = plt.figure()    \n",
    "    gs1 = gridspec.GridSpec(1, 1)\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    cs = ax.tricontourf(xx[:, SNAP], yy[:, SNAP], triang, Z[:, SNAP], 100, cmap=colormaps['jet'])\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    cbar = plt.colorbar(cs, cax=cax)\n",
    "    tick_locator = MaxNLocator(nbins=5)\n",
    "    cbar.locator = tick_locator\n",
    "    cbar.formatter.set_powerlimits((0, 0))\n",
    "    cbar.update_ticks()\n",
    "    plt.tight_layout()\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_title('Solution field for $\\mu$ = '+str(np.around(params[SNAP].detach().numpy(), 2)))\n",
    "    plt.savefig('plots/field_'+str(SNAP)+'_train'+joined+'_test'+fname+'.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "snapshots = np.arange(params[test_trajs].shape[0]).tolist()\n",
    "np.random.shuffle(snapshots)\n",
    "\n",
    "for i in range(len(test_mesh_names)):\n",
    "    print('-'*40)\n",
    "    print(f'TEST MESH: {test_mesh_names[i]}')\n",
    "    scale, U = get_scaled_data(test_solution_names[i])\n",
    "    U = U.to('cpu')\n",
    "    xx = scipy.io.loadmat(\"dataset/matrix\"+test_fidelities[i])['xx']\n",
    "    yy = scipy.io.loadmat(\"dataset/matrix\"+test_fidelities[i])['yy']\n",
    "    triang = scipy.io.loadmat(\"dataset/matrix\"+test_fidelities[i])['T'].astype(int) - 1\n",
    "    df = pd.read_csv(\"dataset/\"+test_mesh_names[i], header=None).values\n",
    "\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    Z, Z_net, x_enc, x_map = evaluate_results(model, df, U, scale, params.to('cpu'))\n",
    "    error = abs(Z - Z_net)\n",
    "    rel_error = print_results(Z, Z_net, x_enc, x_map)\n",
    "\n",
    "    for SNAP in snapshots[0:N]:\n",
    "        if i == 0: plot_fields(SNAP, Z, xx, yy, triang, params.to('cpu'), test_fidelities[i]+\"_solution\")\n",
    "        plot_fields(SNAP, Z_net, xx, yy, triang, params.to('cpu'), test_fidelities[i]+\"_GCA\")\n",
    "        plot_fields(SNAP, error, xx, yy, triang, params.to('cpu'), test_fidelities[i]+\"_error\")\n",
    "    np.savetxt('errors/relative_errors_train'+joined+'_test'+test_fidelities[i]+'.txt', [max(rel_error), sum(rel_error)/len(rel_error), min(rel_error)])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
