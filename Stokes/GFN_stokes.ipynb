{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18472d4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykdtree.kdtree import KDTree\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import colormaps\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from itertools import product\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "plt_params = {'legend.fontsize': 'x-large',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(plt_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9055c8",
   "metadata": {},
   "source": [
    "# Hyperparameter and Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! SETUP !!\n",
    "\n",
    "# Problem to consider\n",
    "# Either \"stokes\", \"advection\" or \"graetz\"\n",
    "pname = 'stokes'\n",
    "name_mesh = 'reference_mesh'\n",
    "name_matrix = 'matrix'\n",
    "\n",
    "# Training data\n",
    "# Note: model assumes data equally split among different fidelities\n",
    "# Loss is computed as the sum of the mse losses weighted by number\n",
    "# of samples\n",
    "train_fidelities = ['_h3', '_h1']\n",
    "\n",
    "#Â test_fidelities = ['_large', '_medium', '_small', '_tiny']\n",
    "test_fidelities = ['_h3', '_h2', '_h1', '_h0']\n",
    "\n",
    "train_mesh_names = [name_mesh+''.join(f)+'.npy' for f in train_fidelities]\n",
    "train_solution_names = [name_matrix+''.join(f)+'.mat' for f in train_fidelities]\n",
    "assert len(train_mesh_names) == len(train_solution_names)\n",
    "\n",
    "# Testing meshes\n",
    "test_mesh_names = [name_mesh+''.join(f)+'.npy' for f in test_fidelities]\n",
    "test_solution_names = [name_matrix+''.join(f)+'.mat' for f in test_fidelities]\n",
    "assert len(train_mesh_names) == len(train_solution_names)\n",
    "\n",
    "# Naming convention for saving the model\n",
    "joined = ''.join(train_fidelities)\n",
    "\n",
    "save_name = 'best_model'+joined+'.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f582d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! MODEL INITIALISATION !!\n",
    "\n",
    "# Set the initial mesh for the model\n",
    "# If intending to do mode 'adaptive' then set equal to largest training mesh\n",
    "# Otherwise, select as desired\n",
    "initial_mesh = train_mesh_names[0]\n",
    "\n",
    "# Select precision\n",
    "# (explanation for when this can matter at https://blog.demofox.org/2017/11/21/floating-point-precision/)\n",
    "precision = torch.float64\n",
    "\n",
    "# Latent dimension\n",
    "latent_size = 20\n",
    "\n",
    "# Mapper sizes\n",
    "# Mapper maps from parameters to latent dimension\n",
    "# We optionally allow the addition of further layers\n",
    "mapper_sizes = [50, 50, 50, 50]\n",
    "\n",
    "# Autoencoder sizes\n",
    "# Autoencoder maps from full data to latent representation\n",
    "# We optionally allow the addition of further layers\n",
    "ae_sizes = []\n",
    "gfn_latent_size = 20\n",
    "\n",
    "# Activation\n",
    "act=nn.Tanh\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! TRAINING !!\n",
    "\n",
    "# Use either fixed (\"fixed\"), adaptive (\"adapt\") or precomputed adaptive method (\"preadapt\")\n",
    "mode = 'fixed'\n",
    "\n",
    "# Weight to give to the mapper loss compared to the autoencoder loss\n",
    "mapper_weight = 1e1\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 5000\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# L2 regularisation hyperparamter\n",
    "lambda_ = 10**-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7f5cd",
   "metadata": {},
   "source": [
    "# Setup Training Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_default_dtype(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbce6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to set up problem data\n",
    "def get_params(pname='stokes'):\n",
    "    if pname == 'stokes':\n",
    "        mu_range = [(0.5,1.5), (0.5,1.5), (0.5,1.5), (0.5,1.5), (0.5,1.5), (-np.pi/6,np.pi/6), (-10,10)]\n",
    "        mus = []\n",
    "        n_pts = [2]*(len(mu_range)-1)+[11]\n",
    "        for i in range(len(mu_range)):\n",
    "            mus.append(np.linspace(mu_range[i][0], mu_range[i][1], n_pts[i]))\n",
    "        return torch.tensor(np.array(list(product(*mus))), dtype=precision)\n",
    "    elif pname == 'graetz':\n",
    "        mus = [np.linspace(1., 3., 10), np.linspace(0.01, 0.1, 20)]\n",
    "        mu1, mu2 = np.meshgrid(mus[0], mus[1])\n",
    "        return torch.tensor(np.vstack((mu1.T, mu2.T)).reshape(2, -1).T, dtype=precision)\n",
    "    elif pname == 'advection':\n",
    "        mus = [np.linspace(0., 6., 10), np.linspace(-1.0, 1.0, 10)]\n",
    "        mu1, mu2 = np.meshgrid(mus[0], mus[1])\n",
    "        return torch.tensor(np.vstack((mu1.T, mu2.T)).reshape(2, -1).T, dtype=precision)\n",
    "\n",
    "def scaler_func():\n",
    "    return preprocessing.StandardScaler()\n",
    "\n",
    "def scaling(U):\n",
    "    scaling_fun_1 = scaler_func()\n",
    "    scaling_fun_2 = scaler_func()\n",
    "    scaler_s = scaling_fun_1.fit(U)\n",
    "    temp = torch.t(torch.tensor(scaler_s.transform(U)))\n",
    "    scaler_f = scaling_fun_2.fit(temp)\n",
    "    scaled_data = torch.unsqueeze(torch.t(torch.tensor(scaler_f.transform(temp), dtype=precision)),0).permute(2, 1, 0)\n",
    "    scale = [scaler_s, scaler_f]\n",
    "    return scale, scaled_data[:,:,0]\n",
    "\n",
    "def undo_scaling(U, scale):\n",
    "    scaler_s = scale[0]\n",
    "    scaler_f = scale[1]\n",
    "    rescaled_data = torch.tensor(scaler_s.inverse_transform(torch.t(torch.tensor(scaler_f.inverse_transform(U.detach().numpy().squeeze())))), dtype=precision)\n",
    "    return rescaled_data\n",
    "\n",
    "def get_scaled_data(fname):\n",
    "    U_orig = torch.tensor(scipy.io.loadmat(\"dataset/\"+fname)['U'])\n",
    "    scale, U_sc = scaling(U_orig)\n",
    "    print('reconstruction error', ((U_orig - undo_scaling(U_sc, scale))**2).sum())\n",
    "    return scale, U_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeaf30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameter space we examine\n",
    "params = get_params(pname).to(dev)\n",
    "n_params = params.shape[1]\n",
    "\n",
    "# Load training data\n",
    "meshes_train = [np.load(\"dataset/\"+i) for i in train_mesh_names]\n",
    "sols_train = [get_scaled_data(i)[1].to(dev) for i in train_solution_names]\n",
    "# Assert that meshes and solutions have same numbers of points\n",
    "assert np.mean([meshes_train[i].shape[0] == sols_train[i].shape[1] for i in range(len(meshes_train))]) == 1\n",
    "# Assert that solutions are present for all the parameter realisations\n",
    "assert np.mean([params.shape[0] == i.shape[0] for i in sols_train]) == 1\n",
    "\n",
    "# Split parameter space for training and testing\n",
    "trajs = list(range(params.shape[0]))\n",
    "random.shuffle(trajs)\n",
    "train_trajs, test_trajs = np.array_split(trajs, 2)\n",
    "train_trajs = np.array_split(train_trajs, len(meshes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe62108",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_mesh = np.load(\"dataset/\"+initial_mesh)\n",
    "\n",
    "expand_master = mode == 'adapt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24005f33",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFN_AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing the GFN method for the encoder and decoder architectures.\n",
    "    \"\"\"\n",
    "    def __init__(self, mesh_m, latent_size=20):\n",
    "        super().__init__()\n",
    "        size = mesh_m.shape[0]\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # Set up master mesh and master weights\n",
    "        # NB: cheaper not to make parameters if we're planning on updating sizes\n",
    "        self.We_m = nn.Parameter(torch.empty(size, self.latent_size))\n",
    "        self.be_m = nn.Parameter(torch.empty(self.latent_size))\n",
    "        self.Wd_m = nn.Parameter(torch.empty(self.latent_size, size))\n",
    "        self.bd_m = nn.Parameter(torch.empty(size))\n",
    "        self.mesh_m = mesh_m\n",
    "        \n",
    "        # Initialise weights\n",
    "        self.initialise(self.We_m, self.be_m)\n",
    "        self.initialise(self.Wd_m, self.bd_m)\n",
    "        \n",
    "        # Set up the weight matrices and mesh used for inference\n",
    "        # Note: no self.be_n since we never need to reshape the encoder biases i.e. be_n == be_m in all cases\n",
    "        self.We_n = self.We_m.clone()\n",
    "        self.Wd_n = self.Wd_m.clone()\n",
    "        self.bd_n = self.bd_m.clone()\n",
    "        \n",
    "    def initialise(self, weight, bias):\n",
    "        stdv = 1. / math.sqrt(weight.size(1))\n",
    "        weight.data.uniform_(-stdv, stdv)\n",
    "        bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def reshape_weights(self, mesh_n, update_master=False, expand=True, agglomerate=True, kd_tree_m=None, kd_tree_n=None, nn_m=None, nn_n=None):\n",
    "        \"\"\"\n",
    "        mesh_n: New geometry to evaluate on\n",
    "        update_master: Whether or not to update the master mesh during the reshaping. Must always\n",
    "                        be False during inference. For 'fixed' or 'preadapt' methods, should be\n",
    "                        False. For 'adapt', should be True during training.\n",
    "        expand: Whether or not the new geometry requires expanding the master mesh\n",
    "        agglomerate: Whether or not the new geometry requires agglomerating the master mesh\n",
    "        kd_tree_m: A precomputed kdtree using nodes in the master mesh\n",
    "        kd_tree_n: A precomputed kdtree using nodes in the new mesh\n",
    "        nn_m: List of nearest neighbours for nodes in the master mesh (i.e. indices of nodes in the new mesh)\n",
    "        nn_n: List of nearest neighbours for nodes in the new mesh (i.e. indices of nodes in the master mesh)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find nearest neighbours if we don't already know them\n",
    "        if nn_n is not None:\n",
    "            pass\n",
    "        elif kd_tree_n is not None:\n",
    "            nn_n = kd_tree_m.query(mesh_n, k=1)[1]\n",
    "        else:\n",
    "            kd_tree_m = KDTree(self.mesh_m)\n",
    "            nn_n = kd_tree_m.query(mesh_n, k=1)[1]\n",
    "        if nn_m is not None:\n",
    "            pass\n",
    "        elif kd_tree_n is not None:\n",
    "            nn_m = kd_tree_n.query(self.mesh_m, k=1)[1]\n",
    "        else:\n",
    "            kd_tree_n = KDTree(mesh_n)\n",
    "            nn_m = kd_tree_n.query(self.mesh_m, k=1)[1]\n",
    "\n",
    "        # WE HAVE TWO CASES TO CONSIDER:\n",
    "        # Case 1: We are either in evaluation mode or we are in training with `update_master=False`\n",
    "        # Case 2: We are in training with `update_master=True`\n",
    "        \n",
    "        # How much did we expand or agglomerate?\n",
    "        nodes_added = 0\n",
    "        nodes_combined = 0\n",
    "\n",
    "        # !! EXPANSION !!\n",
    "        if expand:\n",
    "            # Case 1: master mesh isn't updated\n",
    "            if not update_master or not self.training:\n",
    "                # Set up intermediate variable so we do not update We_m\n",
    "                if self.training:\n",
    "                    We_i = self.We_m.clone()\n",
    "                    Wd_i = self.Wd_m.clone()\n",
    "                    bd_i = self.bd_m.clone()\n",
    "                else:\n",
    "                    We_i = self.We_m\n",
    "                    Wd_i = self.Wd_m\n",
    "                    bd_i = self.bd_m\n",
    "            \n",
    "                # Set up counters\n",
    "                count_m = np.zeros(self.mesh_m.shape[0])\n",
    "                size = self.mesh_m.shape[0]\n",
    "\n",
    "                # Loop over each pt_n_{M_n} with its pair pt_m_{M_m} <- pt_n_{M_n}\n",
    "                for pt_n, pt_m in enumerate(nn_n):\n",
    "                    # if NOT pt_m_{M_m} <-> pt_n_{M_n}\n",
    "                    if nn_m[pt_m] != pt_n:\n",
    "                        nodes_added += 1\n",
    "                        count_m[pt_m] += 1\n",
    "\n",
    "                        # Divide encoder weights by number of expansions\n",
    "                        We_i[pt_m] *= count_m[pt_m]/(count_m[pt_m]+1)\n",
    "                        # Store the index of the weight we want\n",
    "                        # so we can update at the end without storing\n",
    "                        # all the nodes to update explictly\n",
    "                        new_row = torch.zeros(1, We_i.shape[1], device=We_i.device)\n",
    "                        new_row[0][0] = pt_m\n",
    "                        We_i = torch.cat((We_i, new_row), dim=0)\n",
    "                        \n",
    "                        # Duplicate weights for decoder\n",
    "                        Wd_i = torch.cat((Wd_i, Wd_i[:, pt_m:pt_m+1]), dim=1)\n",
    "                        bd_i = torch.cat([bd_i, bd_i[pt_m:pt_m+1]])\n",
    "\n",
    "                        # Directly add nearest neighbour link\n",
    "                        nn_m = np.append(nn_m, pt_n)\n",
    "                \n",
    "                # Loop over the nodes we need to update using the index we stored in the first element\n",
    "                for i in range(size, size+nodes_added):\n",
    "                    index = int(We_i[i,0])\n",
    "                    We_i[i] = We_i[index]\n",
    "\n",
    "            # Case 2: master mesh is updated (only happens if model.train())\n",
    "            else:\n",
    "                # We overwrite master mesh so we can ignore gradient tracking\n",
    "                with torch.no_grad():\n",
    "                    # Set up counters\n",
    "                    count_m = np.zeros(self.mesh_m.shape[0])\n",
    "                    nodes_added = 0\n",
    "                    size = self.mesh_m.shape[0]\n",
    "\n",
    "                    # Loop over each pt_n_{M_n} with its pair pt_m_{M_m} <- pt_n_{M_n}\n",
    "                    for pt_n, pt_m in enumerate(nn_n):\n",
    "                        # if NOT pt_m_{M_m} <-> pt_n_{M_n}\n",
    "                        if nn_m[pt_m] != pt_n:\n",
    "                            nodes_added += 1\n",
    "                            count_m[pt_m] += 1\n",
    "\n",
    "                            # Divide encoder weights by number of expansions\n",
    "                            self.We_m[pt_m] *= count_m[pt_m]/(count_m[pt_m]+1)\n",
    "                            # Store the index of the weight we want\n",
    "                            # so we can update at the end without storing\n",
    "                            # all the nodes to update explictly\n",
    "                            new_row = torch.zeros(1, self.We_m.shape[1], device=self.We_m.device)\n",
    "                            new_row[0][0] = pt_m\n",
    "                            self.We_m = torch.cat((self.We_m, new_row), dim=0)\n",
    "                            \n",
    "                            # Duplicate weights for decoder\n",
    "                            self.Wd_m = torch.cat((self.Wd_m, self.Wd_m[:, pt_m:pt_m+1]), dim=1)\n",
    "                            self.bd_m = torch.cat([self.bd_m, self.bd_m[pt_m:pt_m+1]])\n",
    "\n",
    "                            # Directly add nearest neighbour link\n",
    "                            nn_m = np.append(nn_m, pt_n)\n",
    "                            \n",
    "                            # Update master mesh\n",
    "                            self.mesh_m = np.vstack([self.mesh_m, mesh_n[pt_n]])\n",
    "                    \n",
    "                    # Loop over the nodes we need to update using the index we stored in the first element\n",
    "                    for i in range(size, size+nodes_added):\n",
    "                        index = int(self.We_m[i,0])\n",
    "                        self.We_m[i] = self.We_m[index]\n",
    "\n",
    "                We_i = self.We_m.clone()\n",
    "                Wd_i = self.Wd_m.clone()\n",
    "                bd_i = self.bd_m.clone()\n",
    "\n",
    "        # If we don't expand\n",
    "        else:\n",
    "            if self.training:\n",
    "                We_i = self.We_m.clone()\n",
    "                Wd_i = self.Wd_m.clone()\n",
    "                bd_i = self.bd_m.clone()\n",
    "            else:\n",
    "                We_i = self.We_m\n",
    "                Wd_i = self.Wd_m\n",
    "                bd_i = self.bd_m\n",
    "\n",
    "        # !! AGGLOMERATION !!\n",
    "        if agglomerate:\n",
    "            self.We_n = torch.zeros((mesh_n.shape[0], self.We_m.shape[1]), device=self.We_m.device)\n",
    "            self.Wd_n = torch.zeros((self.We_m.shape[1], mesh_n.shape[0]), device=self.We_m.device)\n",
    "            self.bd_n = torch.zeros(mesh_n.shape[0], device=self.We_m.device)\n",
    "\n",
    "            count_n = np.zeros(mesh_n.shape[0])\n",
    "\n",
    "            # Loop over each pt_m_{M_m} with its pair pt_m_{M_m} -> pt_n_{M_n}\n",
    "            for pt_m, pt_n in enumerate(nn_m):\n",
    "\n",
    "                count_n[pt_n]+=1\n",
    "                if count_n[pt_n]>1:\n",
    "                    nodes_combined+=1\n",
    "\n",
    "                # Summation\n",
    "                self.We_n[pt_n] += We_i[pt_m]\n",
    "                # Calculate Mean\n",
    "                self.Wd_n[:, pt_n] = ( (count_n[pt_n]-1)*self.Wd_n[:, pt_n] + Wd_i[:, pt_m] )/count_n[pt_n]\n",
    "                self.bd_n[pt_n] = ( (count_n[pt_n]-1)*self.bd_n[pt_n] + bd_i[pt_m] )/count_n[pt_n]\n",
    "\n",
    "        # If we don't agglomerate        \n",
    "        else:\n",
    "            self.We_n = We_i[nn_m]\n",
    "            self.Wd_n = Wd_i[:,nn_m]\n",
    "            self.bd_n = bd_i[nn_m]\n",
    "            \n",
    "        return nodes_added, nodes_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFN_ROM(nn.Module):\n",
    "    \n",
    "    def __init__(self, mesh_m, gfn_latent_size=20, latent_size=20, n_params=7, act=nn.Tanh, ae_sizes=[], map_sizes=[50]*4):\n",
    "        super().__init__()\n",
    "        self.GFN = GFN_AE(mesh_m, gfn_latent_size)\n",
    "        \n",
    "        self.act = act()\n",
    "        \n",
    "        module_list_enc = []\n",
    "        module_list_dec = []\n",
    "\n",
    "        for i in range(len(ae_sizes)):\n",
    "            module_list_dec.append(self.act)\n",
    "            if i==0:\n",
    "                module_list_enc.append(nn.Linear(gfn_latent_size, ae_sizes[i]))\n",
    "                module_list_dec.append(nn.Linear(ae_sizes[i], gfn_latent_size))\n",
    "            else:\n",
    "                module_list_enc.append(nn.Linear(ae_sizes[i-1], ae_sizes[i]))\n",
    "                module_list_dec.append(nn.Linear(ae_sizes[i], ae_sizes[i-1]))\n",
    "            module_list_enc.append(self.act)\n",
    "        if len(ae_sizes)!=0:\n",
    "            module_list_dec.append(self.act)\n",
    "            module_list_enc.append(nn.Linear(ae_sizes[-1], latent_size))\n",
    "            module_list_dec.append(nn.Linear(latent_size, ae_sizes[-1]))\n",
    "            module_list_enc.append(self.act)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*module_list_enc)\n",
    "        self.decoder = nn.Sequential(*module_list_dec[::-1])\n",
    "        \n",
    "        module_list_map = []\n",
    "        \n",
    "        for i in range(len(map_sizes)):\n",
    "            if i==0:\n",
    "                module_list_map.append(nn.Linear(n_params, map_sizes[i]))\n",
    "            else:\n",
    "                module_list_map.append(nn.Linear(map_sizes[i-1], map_sizes[i]))\n",
    "            module_list_map.append(act())\n",
    "        if len(map_sizes)!=0:\n",
    "            module_list_map.append(nn.Linear(map_sizes[-1], latent_size))\n",
    "            \n",
    "        self.mapper = nn.Sequential(*module_list_map)\n",
    "        \n",
    "    def forward(self, x, mesh_n, params, update_master=False, expand=True, agglomerate=True, kd_tree_m=None, kd_tree_n=None, nn_m=None, nn_n=None):\n",
    "        n_exp, n_agg = self.GFN.reshape_weights(mesh_n, update_master, expand, agglomerate, kd_tree_m, kd_tree_n, nn_m, nn_n)\n",
    "        \n",
    "        x_enc = self.act(x@self.GFN.We_n+self.GFN.be_m)\n",
    "        x_enc = self.encoder(x_enc)\n",
    "        \n",
    "        x_map = self.mapper(params)\n",
    "        \n",
    "        x_recon = self.decoder(x_enc)\n",
    "        x_recon = x_recon@self.GFN.Wd_n+self.GFN.bd_n\n",
    "        \n",
    "        return x_recon, x_enc, x_map, n_exp, n_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5ba89",
   "metadata": {},
   "source": [
    "# Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GFN_ROM(start_mesh, gfn_latent_size, latent_size, n_params, act, ae_sizes, mapper_sizes).to(dev)\n",
    "print(model.GFN.mesh_m.shape)\n",
    "\n",
    "# We do all of the possible expansions apriori in the preadaptive case\n",
    "# This is a preprocessing step so we don't do any speedup steps here\n",
    "if mode=='preadapt':\n",
    "    count = 0\n",
    "    while count!=0:\n",
    "        count = 0\n",
    "        for mesh_n in meshes_train:\n",
    "            n_exp, n_agg = model.GFN.reshape_weights(mesh_n, update_master=True)\n",
    "            count += n_exp\n",
    "    print(model.GFN.mesh_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not expand_master:\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_)\n",
    "else:\n",
    "    # Cannot update GFN parameters using Adam anymore since we use adaptive method\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_)\n",
    "    \n",
    "    raise Exception(\"Have not yet implemented GD for the GFN parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40bd139",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(\"models/\"+save_name))\n",
    "    print(\"Loading saved network\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Training network\")\n",
    "    best_loss = np.inf\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "\n",
    "    def criterion(x, x_recon, x_enc, x_map):\n",
    "        return nn.functional.mse_loss(x, x_recon)+1e1*nn.functional.mse_loss(x_enc, x_map)\n",
    "\n",
    "    # Find the initial nn lists\n",
    "    kd_tree_m = KDTree(model.GFN.mesh_m)\n",
    "    nn_ns = [kd_tree_m.query(mesh, k=1)[1].astype('int') for mesh in meshes_train]\n",
    "    nn_ms = [KDTree(mesh).query(model.GFN.mesh_m, k=1)[1].astype('int') for mesh in meshes_train]\n",
    "\n",
    "    check_if_expanded = True\n",
    "    # List to check if we need to agglomerate or expand\n",
    "    need_agg = [True]*len(meshes_train)\n",
    "    need_exp = [True]*len(meshes_train)\n",
    "    \n",
    "    loop = tqdm(range(epochs))\n",
    "    for i in loop:\n",
    "        # Put in training mode and reset gradients\n",
    "        model.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        params_train = params[train_trajs[0]]\n",
    "        n_expansions = 0\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for j in range(len(meshes_train)):\n",
    "            # Load data\n",
    "            U_train = sols_train[j][train_trajs[j]]\n",
    "            params_train = params[train_trajs[j]]\n",
    "            mesh_n = meshes_train[j]\n",
    "            # Predict\n",
    "            x_recon, x_enc, x_map, n_exp, n_agg = model(U_train, mesh_n, params_train, update_master=expand_master, nn_m=nn_ms[j], nn_n=nn_ns[j], expand=need_exp[j], agglomerate=need_agg[j])\n",
    "            \n",
    "            # If our master mesh is still changing, check if we had to expand or agglomerate this iteration\n",
    "            # And update whether or not we will have to for future epochs accordingly\n",
    "            if check_if_expanded:\n",
    "                if n_exp == 0:\n",
    "                    need_exp[j] = False\n",
    "                if n_agg == 0:\n",
    "                    need_agg[j] = False\n",
    "            \n",
    "            n_expansions += n_exp\n",
    "            loss += criterion(U_train, x_recon, x_enc, x_map) * mesh_n.shape[0]\n",
    "            \n",
    "        # If we had an expansion, our master mesh changed and therefore we need to be be sure to expand and agglomerate\n",
    "        # and also to update nearest neighbour lists due to this large master mesh\n",
    "        # If there was no expansion, there will be no expansions for any future epochs so we stop checking\n",
    "        if n_expansions > 0 and check_if_expanded and expand_master:\n",
    "            kd_tree_m = KDTree(model.GFN.mesh_m)\n",
    "            nn_ns = [kd_tree_m.query(mesh, k=1)[1].astype('int') for mesh in meshes_train]\n",
    "            nn_ms = [KDTree(mesh).query(model.GFN.mesh_m, k=1)[1].astype('int') for mesh in meshes_train]\n",
    "            need_agg = [True] * len(meshes_train)\n",
    "            need_exp = [True] * len(meshes_train)\n",
    "        else:\n",
    "            check_if_expanded = False\n",
    "        \n",
    "        loss /= np.sum([k.shape[0] for k in meshes_train])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss = loss.item()\n",
    "        \n",
    "        # Put model in evaluation mode for testing and don't track gradients here\n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            params_test = params[test_trajs]\n",
    "\n",
    "            for j in range(len(meshes_train)):\n",
    "                U_test = sols_train[j][test_trajs]\n",
    "                mesh_n = meshes_train[j]\n",
    "                \n",
    "                x_recon, x_enc, x_map, _, _ = model(U_test, mesh_n, params_test, update_master=expand_master, nn_m=nn_ms[j], nn_n=nn_ns[j], expand=need_exp[j], agglomerate=need_agg[j])\n",
    "                \n",
    "                test_loss += criterion(U_test, x_recon, x_enc, x_map).item() * mesh_n.shape[0]\n",
    "            \n",
    "            test_loss /= np.sum([k.shape[0] for k in meshes_train])\n",
    "        \n",
    "        loop.set_postfix({\"Loss(training)\": train_loss, \"Loss(testing)\": test_loss})\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        if test_loss<best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_epoch = i\n",
    "            torch.save(model.state_dict(), \"models/\"+save_name)\n",
    "\n",
    "    model.load_state_dict(torch.load(\"models/\"+save_name))\n",
    "    \n",
    "    # plot training losses\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.plot(test_losses, label='test')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.savefig('plots/loss_plot'+joined+'.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc201167",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GFN_ROM(start_mesh, gfn_latent_size, latent_size, n_params, act, ae_sizes, mapper_sizes).to(dev)\n",
    "# print(model.GFN.mesh_m.shape)\n",
    "\n",
    "# # We do all of the possible expansions apriori in the preadaptive case\n",
    "# # This is a preprocessing step so we don't do any speedup steps here\n",
    "# if mode=='preadapt':\n",
    "#     count = 0\n",
    "#     while count!=0:\n",
    "#         count = 0\n",
    "#         for mesh_n in meshes_train:\n",
    "#             n_exp, n_agg = model.GFN.reshape_weights(mesh_n, update_master=True)\n",
    "#             count += n_exp\n",
    "#     print(model.GFN.mesh_m.shape)\n",
    "    \n",
    "# model.load_state_dict(torch.load(\"models/\"+save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(model, df_large, U_large, scale, params):\n",
    "    with torch.no_grad():\n",
    "        x_recon, x_enc, x_map, _, _  = model(U_large, df_large, params)\n",
    "        x_rom = model.decoder(x_map)\n",
    "        x_rom = x_rom@model.GFN.Wd_n + model.GFN.bd_n\n",
    "        Z = undo_scaling(U_large, scale)\n",
    "        Z_net = undo_scaling(x_rom, scale)\n",
    "    return Z, Z_net, x_enc, x_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9461ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(Z, Z_net, x_enc, x_map):\n",
    "    error_abs_list = list()\n",
    "    norm_z_list = list()\n",
    "    latents_error = list()\n",
    "\n",
    "    for snap in range(Z.shape[1]):\n",
    "        error_abs = np.linalg.norm(abs(Z[:, snap] - Z_net[:, snap]))\n",
    "        norm_z = np.linalg.norm(Z[:, snap], 2)\n",
    "        error_abs_list.append(error_abs)\n",
    "        norm_z_list.append(norm_z)\n",
    "        lat_err = np.linalg.norm(x_enc[snap] - x_map[snap])/np.linalg.norm(x_enc[snap])\n",
    "        latents_error.append(lat_err)\n",
    "\n",
    "    latents_error = np.array(latents_error)\n",
    "    print(\"\\nMaximum relative error for latent  = \", max(latents_error))\n",
    "    print(\"Mean relative error for latent = \", sum(latents_error)/len(latents_error))\n",
    "    print(\"Minimum relative error for latent = \", min(latents_error))\n",
    "\n",
    "    error = np.array(error_abs_list)\n",
    "    norm = np.array(norm_z_list)\n",
    "    rel_error = error/norm\n",
    "    print(\"\\nMaximum absolute error for field \"+\" = \", max(error))\n",
    "    print(\"Mean absolute error for field \"+\" = \", sum(error)/len(error))\n",
    "    print(\"Minimum absolute error for field \"+\" = \", min(error))\n",
    "    print(\"\\nMaximum relative error for field \"+\" = \", max(rel_error))\n",
    "    print(\"Mean relative error for field \"+\" = \", sum(rel_error)/len(rel_error))\n",
    "    print(\"Minimum relative error for field \"+\" = \", min(rel_error))\n",
    "    return rel_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fields(SNAP, Z, xx, yy, triang, params, fname):\n",
    "    fig = plt.figure()    \n",
    "    gs1 = gridspec.GridSpec(1, 1)\n",
    "    ax = plt.subplot(gs1[0, 0])\n",
    "    cs = ax.tricontourf(xx[:, SNAP], yy[:, SNAP], triang, Z[:, SNAP], 100, cmap=colormaps['jet'])\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "    cbar = plt.colorbar(cs, cax=cax)\n",
    "    tick_locator = MaxNLocator(nbins=5)\n",
    "    cbar.locator = tick_locator\n",
    "    cbar.formatter.set_powerlimits((0, 0))\n",
    "    cbar.update_ticks()\n",
    "    plt.tight_layout()\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_title('Solution field for $\\mu$ = '+str(np.around(params[SNAP].detach().numpy(), 2)))\n",
    "    plt.savefig('plots/field_'+str(SNAP)+'_train'+joined+'_test'+fname+'.png', bbox_inches='tight', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c70583",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "snapshots = np.arange(params[test_trajs].shape[0]).tolist()\n",
    "np.random.shuffle(snapshots)\n",
    "\n",
    "for i in range(len(test_mesh_names)):\n",
    "    print('-'*40)\n",
    "    print(f'TEST MESH: {test_mesh_names[i]}')\n",
    "    scale, U = get_scaled_data(test_solution_names[i])\n",
    "    U = U.to('cpu')\n",
    "    # xx = scipy.io.loadmat(\"dataset/matrix\"+test_fidelities[i])['xx']\n",
    "    # yy = scipy.io.loadmat(\"dataset/matrix\"+test_fidelities[i])['yy']\n",
    "    # triang = scipy.io.loadmat(\"dataset/matrix\"+test_fidelities[i])['T'].astype(int) - 1\n",
    "    df = np.load(\"dataset/\"+test_mesh_names[i])\n",
    "\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    Z, Z_net, x_enc, x_map = evaluate_results(model, df, U, scale, params.to('cpu'))\n",
    "    error = abs(Z - Z_net)\n",
    "    rel_error = print_results(Z, Z_net, x_enc, x_map)\n",
    "\n",
    "    # for SNAP in snapshots[0:N]:\n",
    "    #     if i == 0: plot_fields(SNAP, Z, xx, yy, triang, params.to('cpu'), test_fidelities[i]+\"_solution\")\n",
    "    #     plot_fields(SNAP, Z_net, xx, yy, triang, params.to('cpu'), test_fidelities[i]+\"_GCA\")\n",
    "    #     plot_fields(SNAP, error, xx, yy, triang, params.to('cpu'), test_fidelities[i]+\"_error\")\n",
    "    np.savetxt('errors/relative_errors_train'+joined+'_test'+test_fidelities[i]+'.txt', [max(rel_error), sum(rel_error)/len(rel_error), min(rel_error)])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
