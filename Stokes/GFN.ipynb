{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stokes Problem: Training with both Medium & Small Meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc115edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pykdtree.kdtree import KDTree\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "dev = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9f867",
   "metadata": {},
   "source": [
    "## THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60269142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFN_AE(nn.Module):\n",
    "    \"\"\"\n",
    "    Module implementing the GFN method for the encoder and decoder architectures.\n",
    "    Methods:\n",
    "    __init__: initialises the master mesh, and master weight & biases for the 1st and last layer of the encoder and decoder, respectively\n",
    "    expand: add all new expansive nodes in the new mesh to the weight matrices\n",
    "    agglomerate: agglomerate nodes to fit the new mesh\n",
    "    encoder: execute the encoder part\n",
    "    decoder: execute the decoder part\n",
    "    \"\"\"\n",
    "    def __init__(self, mesh_m, latent_size=20):\n",
    "        super().__init__()\n",
    "        size = mesh_m.shape[0]\n",
    "        self.latent_size = latent_size\n",
    "        self.We_m = nn.Parameter(torch.empty(size, self.latent_size))\n",
    "        self.be_m = nn.Parameter(torch.empty(self.latent_size))\n",
    "        self.Wd_m = nn.Parameter(torch.empty(self.latent_size, size))\n",
    "        self.bd_m = nn.Parameter(torch.empty(size))\n",
    "        self.mesh_m = mesh_m\n",
    "        \n",
    "        self.initialise(self.We_m, self.be_m)\n",
    "        self.initialise(self.Wd_m, self.bd_m)\n",
    "        \n",
    "        # Note: no self.be_n since we never need to reshape the encoder biases i.e. be_n == be_m in all cases\n",
    "        self.We_n = self.We_m.clone()\n",
    "        self.Wd_n = self.Wd_m.clone()\n",
    "        self.bd_n = self.bd_m.clone()\n",
    "        self.mesh_n = self.mesh_m\n",
    "        \n",
    "    def initialise(self, weight, bias):\n",
    "        stdv = 1. / math.sqrt(weight.size(1))\n",
    "        weight.data.uniform_(-stdv, stdv)\n",
    "        bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def expand(self, mesh_n, kd_tree_m=None, kd_tree_n=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Expand the new weights and biases with the new expansive nodes. If during training, update the master weights, biases & mesh.\n",
    "        If during inference, then just apply the new ones.\n",
    "        \"\"\"\n",
    "        \n",
    "        # During evaluation, we will have fixed mesh_m so we don't want to recompute every time => option to pass in\n",
    "        if kd_tree_m is None:\n",
    "            kd_tree_m = KDTree(self.mesh_m)\n",
    "        # During training, we will have fixed fidelities that we know and don't need to always recompute\n",
    "        if kd_tree_n is None:\n",
    "            kd_tree_n = KDTree(mesh_n)\n",
    "            \n",
    "        nn_n = kd_tree_m.query(mesh_n, k=1)[1]\n",
    "        nn_m = kd_tree_n.query(self.mesh_m, k=1)[1]\n",
    "\n",
    "        if not self.training:\n",
    "            # ! testing mode !\n",
    "            \n",
    "            # could assign with .data calls here but can't do for train https://discuss.pytorch.org/t/function-tbackward-returned-an-invalid-gradient-at-index-0-got-1-3-but-expected-shape-compatible-with-1-2/125259\n",
    "            \n",
    "            count_m = np.zeros(self.mesh_m.shape[0]) # track how many neighbours\n",
    "            nodes_added = 0 # how much did we increase our master mesh\n",
    "\n",
    "            for pt_n, pt_m in enumerate(nn_n):\n",
    "                if nn_m[pt_m]!=pt_n: # if not bidirectional link <->\n",
    "                    nodes_added += 1\n",
    "                    self.mesh_n = np.vstack([self.mesh_n, mesh_n[pt_n]])\n",
    "                    count_m[pt_m]+=1\n",
    "\n",
    "                    # Divide encoder weights by number of expansions\n",
    "                    self.We_n[pt_m]*=count_m[pt_m]/(count_m[pt_m]+1)\n",
    "                    # Store the index of the weight we want\n",
    "                    # so we can update at the end without storing\n",
    "                    # all the nodes to update explictly\n",
    "                    new_row = torch.zeros(1, self.We_n.shape[1])\n",
    "                    new_row[0][0] = pt_m\n",
    "                    self.We_n = torch.cat((self.We_n, new_row), dim=0)\n",
    "\n",
    "                    # Duplicate weights for decoder\n",
    "                    self.Wd_n = torch.cat((self.Wd_n, self.Wd_n[:, pt_m:pt_m+1]), dim=1)\n",
    "                    self.bd_n = torch.cat([self.bd_n, self.bd_n[pt_m:pt_m+1]])\n",
    "\n",
    "            # Loop over the nodes we need to update using the index we stored in the first element\n",
    "            for i in range(self.mesh_m.shape[0], self.mesh_m.shape[0]+nodes_added):\n",
    "                index = int(self.We_n[i,0])\n",
    "                self.We_n[i] = self.We_n[index]\n",
    "        else:\n",
    "            # ! training mode !\n",
    "            \n",
    "            # Expansion step is essentially creating new weights from scratch\n",
    "            # => ignore gradients and therefore allow for slicing on leaf tensor as required\n",
    "            # (Unless we do something smarter with the gradient tree to track what we're doing...)\n",
    "            with torch.no_grad():\n",
    "                count_m = np.zeros(self.mesh_m.shape[0]) # track how many neighbours\n",
    "                nodes_added = 0 # how much did we increase our master mesh\n",
    "                size=self.mesh_m.shape[0]\n",
    "\n",
    "                for pt_n, pt_m in enumerate(nn_n):\n",
    "                    if nn_m[pt_m]!=pt_n: # if not bidirectional link <->\n",
    "                        nodes_added += 1\n",
    "                        self.mesh_m=np.vstack([self.mesh_m, mesh_n[pt_n]])\n",
    "                        count_m[pt_m]+=1\n",
    "                        \n",
    "                        # Divide encoder weights by number of expansions\n",
    "                        self.We_m[pt_m]*=count_m[pt_m]/(count_m[pt_m]+1)\n",
    "                        # Store the index of the weight we want\n",
    "                        # so we can update at the end without storing\n",
    "                        # all the nodes to update explictly\n",
    "                        new_row = torch.zeros(1, self.We_m.shape[1])\n",
    "                        new_row[0][0] = pt_m\n",
    "                        self.We_m = nn.Parameter(torch.cat((self.We_m, new_row), dim=0))\n",
    "                        \n",
    "                        # Duplicate weights for decoder\n",
    "                        self.Wd_m = nn.Parameter(torch.cat((self.Wd_m, self.Wd_m[:, pt_m:pt_m+1]), dim=1))\n",
    "                        self.bd_m = nn.Parameter(torch.cat([self.bd_m, self.bd_m[pt_m:pt_m+1]]))\n",
    "                \n",
    "                # Loop over the nodes we need to update using the index we stored in the first element\n",
    "                for i in range(size, size+nodes_added):\n",
    "                    index = int(self.We_m[i,0])\n",
    "                    self.We_m[i] = self.We_m[index]\n",
    "            \n",
    "            # now we need to track gradients from our new master weights\n",
    "            self.We_n = self.We_m.clone()\n",
    "            self.Wd_n = self.Wd_m.clone()\n",
    "            self.bd_n = self.bd_m.clone()\n",
    "        return nodes_added\n",
    "        \n",
    "    def agglomerate(self, mesh_n, kd_tree_n=None, nn_m=None, **kwargs):\n",
    "        \n",
    "        # known during training\n",
    "        if nn_m is not None:\n",
    "            pass\n",
    "        # already found in expansion or already known during training\n",
    "        elif kd_tree_n is not None:\n",
    "            nn_m = kd_tree_n.query(self.mesh_n, k=1)[1]\n",
    "        else:\n",
    "            kd_tree_n = KDTree(mesh_n)\n",
    "            nn_m = kd_tree_n.query(self.mesh_n, k=1)[1]\n",
    "\n",
    "        # FIND NEW WEIGHTS (AGGLOMERATIVE ONLY NOW)\n",
    "        We_n = torch.zeros((mesh_n.shape[0], self.We_n.shape[1]), device=self.We_n.device)\n",
    "        Wd_n = torch.zeros((self.We_n.shape[1], mesh_n.shape[0]), device=self.We_n.device)\n",
    "        bd_n = torch.zeros(mesh_n.shape[0], device=self.We_n.device)\n",
    "\n",
    "        count_n = np.zeros(mesh_n.shape[0])\n",
    "\n",
    "        for m, pt_m in enumerate(self.mesh_n): # agglomerate\n",
    "            nn_n = nn_m[m]\n",
    "            count_n[nn_n]+=1\n",
    "            We_n[nn_n] += self.We_n[m]\n",
    "            Wd_n[:, nn_n] = ( (count_n[nn_n]-1)*Wd_n[:, nn_n] + self.Wd_n[:, m] )/count_n[nn_n]\n",
    "            bd_n[nn_n] = ( (count_n[nn_n]-1)*bd_n[nn_n] + self.bd_n[m] )/count_n[nn_n]\n",
    "          \n",
    "        self.We_n = We_n\n",
    "        self.Wd_n = Wd_n\n",
    "        self.bd_n = bd_n\n",
    "        \n",
    "    def reset(self):\n",
    "        self.We_n = self.We_m.clone()\n",
    "        self.Wd_n = self.Wd_m.clone()\n",
    "        self.bd_n = self.bd_m.clone()\n",
    "        if not self.training:\n",
    "            self.mesh_n = self.mesh_m\n",
    "        \n",
    "    def encoder(self, x, mesh_n, exp_enc=True, agg_enc=True, reset_enc=False, **kwargs):\n",
    "        if reset_enc:\n",
    "            self.reset()\n",
    "        if exp_enc:\n",
    "            self.expand(mesh_n, **kwargs)\n",
    "        if agg_enc:\n",
    "            self.agglomerate(mesh_n, **kwargs)\n",
    "        return x@self.We_n+self.be_m\n",
    "        \n",
    "    def decoder(self, x, mesh_n, exp_dec=True, agg_dec=True, reset_dec=False, **kwargs):\n",
    "        if reset_dec:\n",
    "            self.reset()\n",
    "        if exp_dec:\n",
    "            self.expand(mesh_n, **kwargs)\n",
    "        if agg_dec:\n",
    "            self.agglomerate(mesh_n, **kwargs)\n",
    "        return x@self.Wd_n+self.bd_n\n",
    "    \n",
    "class GCA(nn.Module):\n",
    "    \n",
    "    def __init__(self, mesh_m, gfn_latent_size=20, latent_size=20, n_params=7, act=nn.Tanh, ae_sizes=[], map_sizes=[50]*4):\n",
    "        super().__init__()\n",
    "        self.GFN = GFN_AE(mesh_m, gfn_latent_size)\n",
    "        \n",
    "        self.act = act()\n",
    "        \n",
    "        module_list_enc = []\n",
    "        module_list_dec = []\n",
    "\n",
    "        for i in range(len(ae_sizes)):\n",
    "            module_list_dec.append(self.act)\n",
    "            if i==0:\n",
    "                module_list_enc.append(nn.Linear(gfn_latent_size, ae_sizes[i]))\n",
    "                module_list_dec.append(nn.Linear(ae_sizes[i], gfn_latent_size))\n",
    "            else:\n",
    "                module_list_enc.append(nn.Linear(ae_sizes[i-1], ae_sizes[i]))\n",
    "                module_list_dec.append(nn.Linear(ae_sizes[i], ae_sizes[i-1]))\n",
    "            module_list_enc.append(self.act)\n",
    "        if len(ae_sizes)!=0:\n",
    "            module_list_dec.append(self.act)\n",
    "            module_list_enc.append(nn.Linear(ae_sizes[-1], latent_size))\n",
    "            module_list_dec.append(nn.Linear(latent_size, ae_sizes[-1]))\n",
    "            module_list_enc.append(self.act)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*module_list_enc)\n",
    "        self.decoder = nn.Sequential(*module_list_dec[::-1])\n",
    "        \n",
    "        module_list_map = []\n",
    "        \n",
    "        for i in range(len(map_sizes)):\n",
    "            if i==0:\n",
    "                module_list_map.append(nn.Linear(n_params, map_sizes[i]))\n",
    "            else:\n",
    "                module_list_map.append(nn.Linear(map_sizes[i-1], map_sizes[i]))\n",
    "            module_list_map.append(act())\n",
    "        if len(map_sizes)!=0:\n",
    "            module_list_map.append(nn.Linear(map_sizes[-1], latent_size))\n",
    "            \n",
    "        self.mapper = nn.Sequential(*module_list_map)\n",
    "        \n",
    "    def forward(self, x, mesh_n, params, **kwargs):\n",
    "        x_enc = self.act(self.GFN.encoder(x, mesh_n, **kwargs))\n",
    "        x_enc = self.encoder(x_enc)\n",
    "        \n",
    "        x_map = self.mapper(params)\n",
    "        \n",
    "        x_recon = self.decoder(x_enc)\n",
    "        x_recon = self.GFN.decoder(x_enc, mesh_n, **kwargs)\n",
    "        \n",
    "        return x_recon, x_enc, x_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b67f68",
   "metadata": {},
   "source": [
    "## SET UP TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826652a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def get_params():\n",
    "    mu_range = [(0.5,1.5), (0.5,1.5), (0.5,1.5), (0.5,1.5), (0.5,1.5), (-np.pi/6,np.pi/6), (-10,10)]\n",
    "    mus = []\n",
    "    n_pts = [2]*(len(mu_range)-1)+[11]\n",
    "    for i in range(len(mu_range)):\n",
    "        mus.append(np.linspace(mu_range[i][0], mu_range[i][1], n_pts[i]))\n",
    "    return torch.tensor(np.array(list(product(*mus)))).float()\n",
    "\n",
    "def scaler_func():\n",
    "    return preprocessing.StandardScaler()\n",
    "\n",
    "def scaling(U):\n",
    "    scaling_fun_1 = scaler_func()\n",
    "    scaling_fun_2 = scaler_func()\n",
    "    scaler_s = scaling_fun_1.fit(U)\n",
    "    temp = torch.t(torch.tensor(scaler_s.transform(U)))\n",
    "    scaler_f = scaling_fun_2.fit(temp)\n",
    "    scaled_data = torch.unsqueeze(torch.t(torch.tensor(scaler_f.transform(temp))),0).permute(2, 1, 0)\n",
    "    scale = [scaler_s, scaler_f]\n",
    "    return scale, scaled_data[:,:,0]\n",
    "\n",
    "def undo_scaling(U, scale):\n",
    "    scaler_s = scale[0]\n",
    "    scaler_f = scale[1]\n",
    "    rescaled_data = torch.tensor(scaler_s.inverse_transform(torch.t(torch.tensor(scaler_f.inverse_transform(U.detach().numpy().squeeze())))))\n",
    "    return rescaled_data\n",
    "\n",
    "def get_scaled_data(fname):\n",
    "    U_orig = torch.tensor(scipy.io.loadmat(fname)['U'])\n",
    "    scale, U_sc = scaling(U_orig)\n",
    "    print('reconstruction error', ((U_orig - undo_scaling(U_sc, scale))**2).sum())\n",
    "    return scale, U_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e87578",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_params().to(dev)\n",
    "\n",
    "df_small = pd.read_csv('reference_mesh_small.csv', header=None).values\n",
    "df_medium = pd.read_csv('reference_mesh.csv', header=None).values\n",
    "\n",
    "scale_small, U_small = get_scaled_data('matrix_small.mat')\n",
    "U_small = U_small.float().to(dev)\n",
    "scale_medium, U_medium = get_scaled_data('matrix.mat')\n",
    "U_medium = U_medium.float().to(dev)\n",
    "\n",
    "train_trajs = random.sample(range(704), 352)\n",
    "train_trajs_s = train_trajs[:176]\n",
    "train_trajs_m = train_trajs[176:]\n",
    "test_trajs = list(set(range(704)) - set(train_trajs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6adfe",
   "metadata": {},
   "source": [
    "## TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation\n",
    "# NB - important to start with largest mesh since this is better for initialisation purposes\n",
    "# and potential savings in no expansions are done\n",
    "model = GCA(df_medium).to(dev)\n",
    "\n",
    "# conduct the expansion step\n",
    "added_nodes = model.GFN.expand(df_small)\n",
    "\n",
    "added_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "# now initialise our optimiser after we have conducted ALL expansions\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091eeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "EPOCHS = 5000\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "def criterion(x, x_recon, x_enc, x_map):\n",
    "    return nn.functional.mse_loss(x, x_recon)+1e1*nn.functional.mse_loss(x_enc, x_map)\n",
    "\n",
    "kd_tree_n = KDTree(df_small)\n",
    "nn_m = kd_tree_n.query(model.GFN.mesh_m, k=1)[1]\n",
    "\n",
    "U_medium_train = U_medium[train_trajs_m]\n",
    "params_medium = params[train_trajs_m]\n",
    "U_small_train = U_small[train_trajs_s]\n",
    "params_small = params[train_trajs_s]\n",
    "\n",
    "U_medium_test = U_medium[test_trajs]\n",
    "params_test = params[test_trajs]\n",
    "U_small_test = U_small[test_trajs]\n",
    "\n",
    "for i in tqdm(range(EPOCHS)):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    # because added_nodes==0, we know we don't need to expand or agglom for the medium mesh\n",
    "    x_recon, x_enc, x_map = model(U_medium_train, df_medium, params_medium, exp_enc=False, exp_dec=False, agg_enc=False, agg_dec=False, reset_enc=True)\n",
    "    loss_medium = criterion(U_medium_train, x_recon, x_enc, x_map)\n",
    "    \n",
    "    # only need to agglom in training since we expanded as a preprocessing step\n",
    "    x_recon, x_enc, x_map = model(U_small_train, df_small, params_small, exp_enc=False, exp_dec=False, agg_dec=False, nn_m=nn_m, reset_enc=True)\n",
    "    loss_small = criterion(U_small_train, x_recon, x_enc, x_map)\n",
    "    \n",
    "    loss = 1/(df_medium.shape[0]+df_small.shape[0])*(df_small.shape[0]*loss_small + df_medium.shape[0]*loss_medium)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_recon, x_enc, x_map = model(U_medium_test, df_medium, params_test, exp_enc=False, exp_dec=False, agg_enc=False, agg_dec=False, reset_enc=True)\n",
    "        test_loss_m = criterion(U_medium_test, x_recon, x_enc, x_map).item()\n",
    "        x_recon, x_enc, x_map = model(U_small_test, df_small, params_test, exp_enc=False, exp_dec=False, agg_dec=False, nn_m=nn_m, reset_enc=True)\n",
    "        test_loss_s = criterion(U_small_test, x_recon, x_enc, x_map).item()\n",
    "        \n",
    "        test_loss = test_loss_m+df_small.shape[0]/df_medium.shape[0]*test_loss_s\n",
    "    \n",
    "    print(f'Epoch {i}: train loss: {train_loss} | test loss: {test_loss} via (s): {test_loss_s} | (m): {test_loss_m}')\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    if test_loss<best_loss:\n",
    "        best_loss=test_loss\n",
    "        best_epoch = i\n",
    "        torch.save(model.state_dict(), \"best_model_sm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adf1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCA(df_medium).to(dev)\n",
    "model.load_state_dict(torch.load(\"best_model_sm.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984248e4",
   "metadata": {},
   "source": [
    "## EVALUATE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721713de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model, df_large, U_large, scale, params):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_recon, x_enc, x_map = model(U_large, df_large, params, reset_enc=True, reset_dec=True)\n",
    "        \n",
    "        x_rom = model.decoder(x_map)\n",
    "        x_rom = model.GFN.decoder(x_rom, df_large, exp_enc=False, exp_dec=False, agg_enc=False, agg_dec=False)\n",
    "\n",
    "        error_abs_list = list()\n",
    "        norm_z_list = list()\n",
    "        latents_error = list()\n",
    "        Z = undo_scaling(U_large, scale)\n",
    "        Z_net = undo_scaling(x_rom, scale)\n",
    "        for snap in range(U_large.shape[0]):\n",
    "            error_abs = np.linalg.norm(abs(Z[:, snap] - Z_net[:, snap]))\n",
    "            norm_z = np.linalg.norm(Z[:, snap], 2)\n",
    "            error_abs_list.append(error_abs)\n",
    "            norm_z_list.append(norm_z)\n",
    "            lat_err = np.linalg.norm(x_enc[snap] - x_map[snap])/np.linalg.norm(x_enc[snap])\n",
    "            latents_error.append(lat_err)\n",
    "\n",
    "        latents_error = np.array(latents_error)\n",
    "        print(\"\\nMaximum relative error for latent  = \", max(latents_error))\n",
    "        print(\"Mean relative error for latent = \", sum(latents_error)/len(latents_error))\n",
    "        print(\"Minimum relative error for latent = \", min(latents_error))\n",
    "\n",
    "        error = np.array(error_abs_list)\n",
    "        norm = np.array(norm_z_list)\n",
    "        rel_error = error/norm\n",
    "        print(\"\\nMaximum absolute error for field \"+\" = \", max(error))\n",
    "        print(\"Mean absolute error for field \"+\" = \", sum(error)/len(error))\n",
    "        print(\"Minimum absolute error for field \"+\" = \", min(error))\n",
    "        print(\"\\nMaximum relative error for field \"+\" = \", max(rel_error))\n",
    "        print(\"Mean relative error for field \"+\" = \", sum(rel_error)/len(rel_error))\n",
    "        print(\"Minimum relative error for field \"+\" = \", min(rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097af20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, U = get_scaled_data('matrix_tiny.mat')\n",
    "U = U.to('cpu').float()\n",
    "df = pd.read_csv('reference_mesh_tiny.csv', header=None).values\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "print_results(model, df, U, scale, params.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, U = get_scaled_data('matrix_small.mat')\n",
    "U = U.to('cpu').float()\n",
    "df = pd.read_csv('reference_mesh_small.csv', header=None).values\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "print_results(model, df, U, scale, params.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9dc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, U = get_scaled_data('matrix.mat')\n",
    "U = U.to('cpu').float()\n",
    "df = pd.read_csv('reference_mesh.csv', header=None).values\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "print_results(model, df, U, scale, params.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, U = get_scaled_data('matrix_large.mat')\n",
    "U = U.to('cpu').float()\n",
    "df = pd.read_csv('reference_mesh_large.csv', header=None).values\n",
    "\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "print_results(model, df, U, scale, params.to('cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
